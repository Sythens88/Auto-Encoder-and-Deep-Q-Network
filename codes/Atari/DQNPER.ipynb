{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN with Atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atari_wrappers import make_atari, wrap_deepmind\n",
    "from PERMemory import Memory\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari('PongNoFrameskip-v4')\n",
    "env = wrap_deepmind(env, scale = False, frame_stack=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, in_channel, hidden_dim, action_space):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channel, 12, kernel_size = 8, stride = 4) \n",
    "        self.conv2 = nn.Conv2d(12, 24, kernel_size = 3, stride = 2)\n",
    "        self.conv3 = nn.Conv2d(24, 24, kernel_size = 3, stride = 1)\n",
    "        self.fc1 = nn.Linear(7*7*24, 256)\n",
    "        self.fc2 = nn.Linear(256, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_space)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) ## bs*19*19*12\n",
    "        x = F.relu(self.conv2(x)) ## bs*9*9*24\n",
    "        x = F.relu(self.conv3(x)) ## bs*7*7*24\n",
    "        x = F.relu(self.fc1(x.view(x.size(0),-1))) ## bs,256\n",
    "        x = F.relu(self.fc2(x)) ## bs,hidden_dim\n",
    "        x = self.fc3(x) ## bs,action_space\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agent\n",
    "\n",
    "class DQNAgent():\n",
    "    def __init__(self, env, hidden_dim, capacity, batch_size, in_channel = 4,\n",
    "                 gamma = 0.9, epsilon = 0.1, decay_rate = 1, learning_rate = 1e-4, init = True):\n",
    "        self.env = env\n",
    "        self.action_space = self.env.action_space  \n",
    "        self.obs_space = self.env.observation_space.shape\n",
    "        self.action_len = len([i for i in range(self.action_space.n)])\n",
    "        self.memory = Memory(capacity = capacity)\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.behaviour_QNetwork = QNetwork(in_channel, hidden_dim, self.action_len).to(self.device)\n",
    "        self.target_QNetwork = QNetwork(in_channel, hidden_dim, self.action_len).to(self.device)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.behaviour_QNetwork.parameters(), lr = learning_rate)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.decay_rate = decay_rate\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        if init:\n",
    "            print(\"********** buffer memory **********\")\n",
    "            for _ in tqdm(range(100)):\n",
    "                s0 = self.env.reset()\n",
    "                s0 = self.stateTran(s0)\n",
    "                is_end = False\n",
    "                while not is_end:\n",
    "                    a0 = self.action_space.sample()\n",
    "                    s1, reward, is_end, _ = self.env.step(a0)\n",
    "                    s1 = self.stateTran(s1)\n",
    "                    self.memory.store([s0,a0,reward,s1,is_end])\n",
    "                    s0 = s1\n",
    "                \n",
    "    def stateTran(self,state):\n",
    "        return state._force().transpose(2,0,1)\n",
    "        \n",
    "    def policy(self, state, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = self.action_space.sample()\n",
    "        else:\n",
    "            score = self.behaviour_QNetwork(torch.Tensor(state).unsqueeze(0).to(self.device)).detach()\n",
    "            action = torch.argmax(score).item()\n",
    "        return action\n",
    "    \n",
    "    def learn(self, display = False):\n",
    "        s0 = self.env.reset()\n",
    "        s0 = self.stateTran(s0)\n",
    "        if display:\n",
    "            self.env.render()\n",
    "        is_end = False\n",
    "        episode_reward = 0\n",
    "        losses = []\n",
    "        \n",
    "        while not is_end:\n",
    "            ## choose an action and make a step\n",
    "            a0 = self.policy(s0, epsilon = self.epsilon)\n",
    "            s1, reward, is_end, _ = self.env.step(a0)\n",
    "            s1 = self.stateTran(s1)\n",
    "            if display:\n",
    "                self.env.render()\n",
    "            \n",
    "            ## store the transition into memory\n",
    "            self.memory.store([s0,a0,reward,s1,is_end])\n",
    "            \n",
    "            ## sample minibatch from memory\n",
    "            b_idx, b_memory, ISWeights = self.memory.sample(self.batch_size)\n",
    "            b_s, b_a, b_r, b_s_next, b_e = [], [], [], [], []\n",
    "            for batch in b_memory:\n",
    "                b_s.append(batch[0])\n",
    "                b_a.append(batch[1])\n",
    "                b_r.append(batch[2])\n",
    "                b_s_next.append(batch[3])\n",
    "                b_e.append(batch[4])\n",
    "            b_s = torch.Tensor(b_s).to(self.device)\n",
    "            b_a = torch.LongTensor(b_a).to(self.device).reshape(-1,1)\n",
    "            b_r = torch.Tensor(b_r).to(self.device).reshape(-1,1)\n",
    "            b_s_next = torch.Tensor(b_s_next).to(self.device)\n",
    "            b_e = torch.Tensor(b_e).to(self.device)\n",
    "            \n",
    "            ## compute two Q values\n",
    "            #Q_target = b_r + self.gamma * torch.max(self.target_QNetwork(b_s_next),1)[0].reshape(-1,1) * (1 - b_e).reshape(-1,1)\n",
    "            max_a = torch.argmax(self.behaviour_QNetwork(b_s_next),1).detach().reshape(-1,1)\n",
    "            Q_target = b_r + self.gamma * self.target_QNetwork(b_s_next).gather(1,max_a) * (1 - b_e).reshape(-1,1)\n",
    "            Q_behaviour = self.behaviour_QNetwork(b_s).gather(1,b_a)\n",
    "            \n",
    "            ## update memory\n",
    "            abs_err = np.abs((Q_behaviour-Q_target).cpu().detach().numpy()).reshape(self.batch_size,)\n",
    "            self.memory.batch_update(b_idx, abs_err)\n",
    "            \n",
    "            # learn\n",
    "            ## compute loss\n",
    "            #loss = self.loss_fn(Q_target, Q_behaviour)\n",
    "            loss = torch.sum(torch.Tensor(ISWeights).to(self.device) * (Q_target - Q_behaviour)**2)\n",
    "            losses.append(loss.item())\n",
    "            ## back prop\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            ## iteration\n",
    "            s0 = s1\n",
    "            episode_reward += reward\n",
    "        \n",
    "        ## update target network\n",
    "        self.target_QNetwork.load_state_dict(self.behaviour_QNetwork.state_dict())\n",
    "        self.epsilon *= self.decay_rate\n",
    "        \n",
    "        return episode_reward, np.mean(losses)\n",
    "    \n",
    "    def save_model(self):\n",
    "        torch.save(self.behaviour_QNetwork, 'saved_model\\DQN')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODE = 50\n",
    "episode_reward = []\n",
    "episode_loss = []\n",
    "\n",
    "agent = DQNAgent(env = env, hidden_dim = 6, capacity = 10000, batch_size = 64, \n",
    "                  gamma = 0.9, epsilon = 0.1, decay_rate = 0.99999, learning_rate = 1e-3, init = True)\n",
    "\n",
    "print(\"**********begin training***********\")\n",
    "\n",
    "for e in tqdm(range(MAX_EPISODE)):\n",
    "    reward, loss = agent.learn(display = False)\n",
    "    episode_reward.append(reward)\n",
    "    episode_loss.append(loss)\n",
    "    if e % 10 == 0:\n",
    "        print(\"episode:\", e, \"reward:\", reward, \"loss:\", round(loss,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_reward)\n",
    "plt.title(\"Pong with PER\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_loss)\n",
    "plt.title(\"loss with PER\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save learning curve\n",
    "dqn_curve = np.array(episode_reward)\n",
    "np.save('curve\\DQN', dqn_curve)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
