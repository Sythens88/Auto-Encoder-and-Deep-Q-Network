{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, random, os, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from atari_wrappers import make_atari, wrap_deepmind,LazyFrames\n",
    "from tqdm import tqdm\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and wrap the environment\n",
    "env = make_atari('PongNoFrameskip-v4')\n",
    "env = wrap_deepmind(env, scale = False, frame_stack=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, num_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4), # bs*32*19*19\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2), # bs*64*9*9\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), # bs*64*7*7\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.encoder_linear = nn.Sequential(\n",
    "            nn.Linear(7 * 7 * 64, 512), # bs*512\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, hidden_dim), # bs*hid_dim\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.DQN = nn.Linear(hidden_dim, num_actions) # bs*num_actions\n",
    "        self.encoder_conv = torch.load('saved_model/daqn_pre_conv')\n",
    "        self.encoder_linear = torch.load('saved_model/daqn_pre_linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## encoder:input->hidden\n",
    "        hidden = self.encoder_conv(x)\n",
    "        hidden = hidden.reshape(hidden.size(0),-1)\n",
    "        hidden = self.encoder_linear(hidden)\n",
    "        ## DQN:hidden->qtable\n",
    "        qtable = self.DQN(hidden)\n",
    "        \n",
    "        return qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, memory_size=100000):\n",
    "        self.buffer = []\n",
    "        self.memory_size = memory_size\n",
    "        self.next_idx = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        data = (state, action, reward, next_state, done)\n",
    "        if len(self.buffer) <= self.memory_size: \n",
    "            self.buffer.append(data)\n",
    "        else: # buffer is full\n",
    "            self.buffer[self.next_idx] = data\n",
    "        self.next_idx = (self.next_idx + 1) % self.memory_size\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAQNAgent: \n",
    "    def __init__(self, in_channels = 1, action_space = [], hidden_dim = 6, USE_CUDA = False, memory_size = 10000, epsilon  = 1, lr = 1e-4):\n",
    "        self.epsilon = epsilon\n",
    "        self.action_space = action_space\n",
    "        self.memory = Memory(memory_size)\n",
    "        self.behaviourNet = QNetwork(in_channels = in_channels, hidden_dim = hidden_dim, num_actions = action_space.n)\n",
    "        self.targetNet = QNetwork(in_channels = in_channels, hidden_dim = hidden_dim, num_actions = action_space.n)\n",
    "        self.targetNet.load_state_dict(self.behaviourNet.state_dict())\n",
    "\n",
    "        self.USE_CUDA = USE_CUDA\n",
    "        if USE_CUDA:\n",
    "            self.behaviourNet = self.behaviourNet.cuda()\n",
    "            self.targetNet = self.targetNet.cuda()\n",
    "        self.optimizer = torch.optim.Adam(self.behaviourNet.parameters(),lr=lr)\n",
    "\n",
    "    def observe(self, lazyframe):\n",
    "        # from Lazy frame to tensor\n",
    "        state =  torch.from_numpy(lazyframe._force().transpose(2,0,1)[None]/255).float()\n",
    "        if self.USE_CUDA:\n",
    "            state = state.cuda()\n",
    "        return state\n",
    "\n",
    "    def value(self, state):\n",
    "        q_values = self.behaviourNet(state)\n",
    "        return q_values\n",
    "    \n",
    "    def act(self, state, epsilon = None):\n",
    "        if epsilon is None: epsilon = self.epsilon\n",
    "        q_values = self.value(state).cpu().detach().numpy()\n",
    "        if random.random() < epsilon:\n",
    "            aciton = random.randrange(self.action_space.n)\n",
    "        else:\n",
    "            aciton = q_values.argmax(1)[0]\n",
    "        return aciton\n",
    "    \n",
    "    def compute_td_loss(self, states, actions, rewards, next_states, is_done, gamma = 0.99):\n",
    "        actions = torch.tensor(actions).long()    # shape: [batch_size]\n",
    "        rewards = torch.tensor(rewards, dtype =torch.float)  # shape: [batch_size]\n",
    "        is_done = torch.tensor(is_done).bool()  # shape: [batch_size]\n",
    "        if self.USE_CUDA:\n",
    "            actions = actions.cuda()\n",
    "            rewards = rewards.cuda()\n",
    "            is_done = is_done.cuda()\n",
    "            \n",
    "        # get q-values for all actions in current states\n",
    "        predicted_qvalues = self.behaviourNet(states)\n",
    "        # select q-values for chosen actions:Q(s_t,a_t)\n",
    "        predicted_qvalues_for_actions = predicted_qvalues[range(states.shape[0]), actions]\n",
    "        # compute q-values for all actions in next states:Q(s_t+1,a*)\n",
    "        predicted_next_qvalues = self.targetNet(next_states)\n",
    "        # compute V*(next_states) using predicted next q-values:max Q(s_t+1,a*)\n",
    "        next_state_values =  predicted_next_qvalues.max(-1)[0] \n",
    "        # compute \"target q-values\" for loss \n",
    "        target_qvalues_for_actions = rewards + gamma *next_state_values\n",
    "        # at the last state we shall use simplified formula: done or not\n",
    "        target_qvalues_for_actions = torch.where(is_done, rewards, target_qvalues_for_actions)\n",
    "\n",
    "        # loss\n",
    "        loss = F.smooth_l1_loss(predicted_qvalues_for_actions, target_qvalues_for_actions.detach())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def sample_from_buffer(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            idx = random.randint(0, self.memory.size() - 1)\n",
    "            data = self.memory.buffer[idx]\n",
    "            frame, action, reward, next_frame, done = data\n",
    "            states.append(self.observe(frame))\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(self.observe(next_frame))\n",
    "            dones.append(done)\n",
    "        return torch.cat(states), actions, rewards, torch.cat(next_states), dones\n",
    "\n",
    "    def learn_from_experience(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = self.sample_from_buffer(batch_size)\n",
    "        td_loss = self.compute_td_loss(states, actions, rewards, next_states, dones)\n",
    "        self.optimizer.zero_grad()\n",
    "        td_loss.backward()\n",
    "        for param in self.behaviourNet.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        return(td_loss.item())\n",
    "    \n",
    "    def save_model(self):\n",
    "        torch.save(self.behaviourNet, 'saved_model\\DAQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HyperParameters\n",
    "gamma = 0.99\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.05\n",
    "eps_decay = 30000\n",
    "frames = 1000000\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "learning_rate = 2e-4\n",
    "max_buff = 100000\n",
    "update_tar_interval = 1000\n",
    "batch_size = 32\n",
    "print_interval = 10000\n",
    "\n",
    "action_space = env.action_space\n",
    "action_dim = env.action_space.n\n",
    "state_channel = env.observation_space.shape[2]\n",
    "hidden_dim = 6\n",
    "agent = DAQNAgent(in_channels = state_channel, action_space = action_space, hidden_dim = hidden_dim, \n",
    "                 USE_CUDA = USE_CUDA, lr = learning_rate, memory_size = max_buff)\n",
    "\n",
    "for _ in tqdm(range(100)):\n",
    "    frame = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = random.randrange(agent.action_space.n)\n",
    "        next_frame, reward, done, _ = env.step(action)\n",
    "        agent.memory.push(frame, action, reward, next_frame, done)\n",
    "        frame = next_frame\n",
    "frame = env.reset()\n",
    "\n",
    "episode_reward = 0\n",
    "all_rewards = []\n",
    "avg_rewards = []\n",
    "losses = []\n",
    "episode_num = 0\n",
    "save_flag = False\n",
    "\n",
    "# e-greedy decay\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_min + (epsilon_max - epsilon_min) * math.exp(\n",
    "            -1. * frame_idx / eps_decay)\n",
    "\n",
    "for i in tqdm(range(frames)):\n",
    "    epsilon = epsilon_by_frame(i)\n",
    "    state_tensor = agent.observe(frame)\n",
    "    action = agent.act(state_tensor, epsilon)\n",
    "    \n",
    "    next_frame, reward, done, _ = env.step(action)\n",
    "    \n",
    "    episode_reward += reward\n",
    "    agent.memory.push(frame, action, reward, next_frame, done)\n",
    "    frame = next_frame\n",
    "    \n",
    "    loss = agent.learn_from_experience(batch_size)\n",
    "    losses.append(loss)\n",
    "\n",
    "    if i % print_interval == 0:\n",
    "        print(\"frames: %5d, reward: %5f, loss: %4f, epsilon: %5f, episode: %4d\" % (i, np.mean(all_rewards[-10:]), loss, epsilon, episode_num))\n",
    "\n",
    "    if i % update_tar_interval == 0:\n",
    "        agent.targetNet.load_state_dict(agent.behaviourNet.state_dict())\n",
    "    \n",
    "    if done:\n",
    "        frame = env.reset()\n",
    "        all_rewards.append(episode_reward)        \n",
    "        episode_reward = 0\n",
    "        episode_num += 1\n",
    "        \n",
    "        avg_reward = np.mean(all_rewards[-50:])\n",
    "        avg_rewards.append(avg_reward)\n",
    "        \n",
    "        if avg_reward > 19:\n",
    "            if save_flag == False:\n",
    "                agent.save_model()\n",
    "                save_flag = True\n",
    "                max_reward = avg_reward\n",
    "                print(\"model saved, episode:\", episode_num, \",avg reward:\", avg_reward)\n",
    "            else:\n",
    "                if avg_reward >= max_reward:\n",
    "                    agent.save_model()\n",
    "                    max_reward = avg_reward\n",
    "                    print(\"model saved, episode:\", episode_num, \",avg reward:\", avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save learning curve\n",
    "learning_curve = np.array(avg_rewards)\n",
    "np.save('curve\\DAQN', learning_curve)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
